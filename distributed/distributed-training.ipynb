{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# distributed-training\n",
    "\n",
    "This tutorial covers distributed training on Spell using our integration with [horovod](https://github.com/horovod/horovod), with a code sample in PyTorch.\n",
    "\n",
    "**Distributed training** is a set of techniques for using GPUs scattered across many different machines for training a machine learning model. `horovod` provides an easy-to-use, performant, cross-platform way of performing distributing model training, making it an essential tool for training very large models.\n",
    "\n",
    "In this tutorial we will demonstrate an MNIST PyTorch model adapted for usage with `horovod`. We will then execute these training scripts on Spell, using our built-in Horovod integration: `spell run --distributed`.\n",
    "\n",
    "Note: for code samples using other frameworks checkout the [examples directory](https://github.com/horovod/horovod/tree/master/examples) in the Horovod GH repo.\n",
    "\n",
    "## prerequisites\n",
    "\n",
    "* An account on [Spell for Teams](https://spell.run/pricing).\n",
    "* The `spell` Python package installed in your local environment. Alternatively, you can launch this notebook from a Spell workspace by running the following CLI command:\n",
    "\n",
    "    ```python\n",
    "    spell jupyter \\\n",
    "        --lab \\\n",
    "        --github-url https://github.com/spellrun/spell-examples.git \\\n",
    "        distributed-workspace\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how it works\n",
    "\n",
    "The simplest practical distributed training strategy is the **parameter server model**. In the parameter-server model, there were typically a number of worker processes and a parameter server (using multiple parameter servers was also possible).\n",
    "\n",
    "Each worker process would be sent a unique batch of data by the parameter server, perform forward and back propogation on that data, and accumulate some gradients. The parameter process would block until it recieves all of the gradient updates from the worker processes, calculate an average gradient, multiply that by the learning rate, and send that back to the worker processes. The worker processes would apply the update before proceeding to the next batch of training.\n",
    "\n",
    "![](https://i.imgur.com/luwOpug.png)\n",
    "\n",
    "Though conceptually simple, this design proved to have significant limitations. The [blog post accompanying the open sourcing of Horovod](https://eng.uber.com/horovod/) does a good job of summarizing the issues. The TLDR Is that the network overhead cost of having so many fan-out connections between worker processes and parameter services pushes real GPU utilization at scale (64+ GPUs) to 50% or less.\n",
    "\n",
    "Horovod borrows instead from the high performance computing world, using the **ring all-reduce algorithm** to handle gradient updates. This scheme does away with parameter servers, using instead a well-defined sequence of cyclical read and write operations to communicate gradient updates between GPUs directly:\n",
    "\n",
    "![](https://i.imgur.com/vXpNPqC.png)\n",
    "\n",
    "In this **all-reduce model** every GPU still has a local copy of the model parameters, and these parameters still need to be kept in sync as training proceeds. But since there are no more parameter servers, network throughput (and API complexity) is much improved.\n",
    "\n",
    "To learn more refer to the [Concepts](https://horovod.readthedocs.io/en/latest/concepts_include.html) page in the Horovod docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code sample\n",
    "\n",
    "Using `curl` to download [sample code](https://raw.githubusercontent.com/horovod/horovod/master/examples/pytorch_mnist.py) for training MNIST using Horovod in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  7708  100  7708    0     0  13788      0 --:--:-- --:--:-- --:--:-- 13788\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/horovod/horovod/master/examples/pytorch_mnist.py > pytorch_mnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load pytorch_mnist.py\n",
    "import argparse\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data.distributed\n",
    "import horovod.torch as hvd\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=42, metavar='S',\n",
    "                    help='random seed (default: 42)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--fp16-allreduce', action='store_true', default=False,\n",
    "                    help='use fp16 compression during allreduce')\n",
    "parser.add_argument('--use-adasum', action='store_true', default=False,\n",
    "                    help='use adasum algorithm to do reduction')\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    # Horovod: set epoch to sampler for shuffling.\n",
    "    train_sampler.set_epoch(epoch)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            # Horovod: use train_sampler to determine the number of examples in\n",
    "            # this worker's partition.\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_sampler),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def metric_average(val, name):\n",
    "    tensor = torch.tensor(val)\n",
    "    avg_tensor = hvd.allreduce(tensor, name=name)\n",
    "    return avg_tensor.item()\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0.\n",
    "    test_accuracy = 0.\n",
    "    for data, target in test_loader:\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        test_accuracy += pred.eq(target.data.view_as(pred)).cpu().float().sum()\n",
    "\n",
    "    # Horovod: use test_sampler to determine the number of examples in\n",
    "    # this worker's partition.\n",
    "    test_loss /= len(test_sampler)\n",
    "    test_accuracy /= len(test_sampler)\n",
    "\n",
    "    # Horovod: average metric values across workers.\n",
    "    test_loss = metric_average(test_loss, 'avg_loss')\n",
    "    test_accuracy = metric_average(test_accuracy, 'avg_accuracy')\n",
    "\n",
    "    # Horovod: print output only on first rank.\n",
    "    if hvd.rank() == 0:\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(\n",
    "            test_loss, 100. * test_accuracy))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parser.parse_args()\n",
    "    args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    # Horovod: initialize library.\n",
    "    hvd.init()\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    if args.cuda:\n",
    "        # Horovod: pin GPU to local rank.\n",
    "        torch.cuda.set_device(hvd.local_rank())\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "\n",
    "    # Horovod: limit # of CPU threads to be used per worker.\n",
    "    torch.set_num_threads(1)\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "    # When supported, use 'forkserver' to spawn dataloader workers instead of 'fork' to prevent\n",
    "    # issues with Infiniband implementations that are not fork-safe\n",
    "    if (kwargs.get('num_workers', 0) > 0 and hasattr(mp, '_supports_context') and\n",
    "            mp._supports_context and 'forkserver' in mp.get_all_start_methods()):\n",
    "        kwargs['multiprocessing_context'] = 'forkserver'\n",
    "\n",
    "    train_dataset = \\\n",
    "        datasets.MNIST('data-%d' % hvd.rank(), train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ]))\n",
    "    # Horovod: use DistributedSampler to partition the training data.\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.batch_size, sampler=train_sampler, **kwargs)\n",
    "\n",
    "    test_dataset = \\\n",
    "        datasets.MNIST('data-%d' % hvd.rank(), train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]))\n",
    "    # Horovod: use DistributedSampler to partition the test data.\n",
    "    test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        test_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.test_batch_size,\n",
    "                                              sampler=test_sampler, **kwargs)\n",
    "\n",
    "    model = Net()\n",
    "\n",
    "    # By default, Adasum doesn't need scaling up learning rate.\n",
    "    lr_scaler = hvd.size() if not args.use_adasum else 1\n",
    "\n",
    "    if args.cuda:\n",
    "        # Move model to GPU.\n",
    "        model.cuda()\n",
    "        # If using GPU Adasum allreduce, scale learning rate by local_size.\n",
    "        if args.use_adasum and hvd.nccl_built():\n",
    "            lr_scaler = hvd.local_size()\n",
    "\n",
    "    # Horovod: scale learning rate by lr_scaler.\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr * lr_scaler,\n",
    "                          momentum=args.momentum)\n",
    "\n",
    "    # Horovod: broadcast parameters & optimizer state.\n",
    "    hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "    hvd.broadcast_optimizer_state(optimizer, root_rank=0)\n",
    "\n",
    "    # Horovod: (optional) compression algorithm.\n",
    "    compression = hvd.Compression.fp16 if args.fp16_allreduce else hvd.Compression.none\n",
    "\n",
    "    # Horovod: wrap optimizer with DistributedOptimizer.\n",
    "    optimizer = hvd.DistributedOptimizer(optimizer,\n",
    "                                         named_parameters=model.named_parameters(),\n",
    "                                         compression=compression,\n",
    "                                         op=hvd.Adasum if args.use_adasum else hvd.Average)\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(epoch)\n",
    "        test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the differences between this \"horovod-ified\" training script and a normal one, let's step through the changes one at a time.\n",
    "\n",
    "```python\n",
    "# Horovod: initialize library.\n",
    "hvd.init()\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if args.cuda:\n",
    "    # Horovod: pin GPU to local rank.\n",
    "    torch.cuda.set_device(hvd.local_rank())\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "```\n",
    "\n",
    "After importing the Horovod PyTorch binding using `import horovod.torch as hvd` we need to call `hvd.init()` to initialize it. All of the state that `horovod` manages will be passed into this script inside of this `hvd` object.\n",
    "\n",
    "In this first bit of initialization we see the first of these local variables: `hvd.local_rank()`. \n",
    "\n",
    "The **local rank** is an ID number assigned to each GPU device on a machine, and it ranges from 0 to `n - 1`, where `n` is the number of GPUs devices on the machine. Horovod launches one copy of this training script for each GPU on the device, so we use `torch.cuda.set_device` to instruct PyTorch to run this code on the specific GPU Horovod has attached this script to.\n",
    "\n",
    "----\n",
    "\n",
    "```python\n",
    "# Horovod: limit # of CPU threads to be used per worker.\n",
    "torch.set_num_threads(1)\n",
    "```\n",
    "\n",
    "PyTorch performs a large number of CPU operations processing training data and moving it to GPU over the course of a training run. This is parallelized across some number of worker processes, each of which uses some number of threads to do its work.\n",
    "\n",
    "`set_num_threads` controls the thread count per worker. It's recommended to set this value to 1 initially to prevent memory saturation (you can relax this restriction later). Cf. [GH#1314](https://github.com/horovod/horovod/pull/1314).\n",
    "\n",
    "----\n",
    "\n",
    "```python\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "# When supported, use 'forkserver' to spawn dataloader workers instead of 'fork' to prevent\n",
    "# issues with Infiniband implementations that are not fork-safe\n",
    "if (kwargs.get('num_workers', 0) > 0 and hasattr(mp, '_supports_context') and\n",
    "        mp._supports_context and 'forkserver' in mp.get_all_start_methods()):\n",
    "    kwargs['multiprocessing_context'] = 'forkserver'\n",
    "```\n",
    "\n",
    "This next bit of code is included due to some leaky abstractions. `kwargs` is a set of keyword arguments to be passed to the PyTorch `DataLoader`. See [GH#1824](https://github.com/horovod/horovod/pull/1824) for details. If you're not using Infiniband, you can safely omit this workaround.\n",
    "\n",
    "----\n",
    "\n",
    "```python\n",
    "# Horovod: use DistributedSampler to partition the training data.\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=args.batch_size, sampler=train_sampler, **kwargs)\n",
    "```\n",
    "\n",
    "The `sampler` component in `DataLoader` returns an iterable of indices from the dataset to be drawn. The default sampler in PyTorch is sequential, returning the sequence 0, 1, 2, ..., n. Horovod overrides this behavior with its `DistributedSampler`, which handles partitioning the dataset across machines. `DistributedSampler` itself takes two parameters as input: `hvd.size()` (the total number of GPUs, e.g. 16) and `hvd.rank()` (the ID assigned to this device from the overall list, e.g. 0...15).\n",
    "\n",
    "Note that the sampler also needs to know the current epoch. `train` calls `train_sampler.set_epoch(epoch)` on every training loop to achieve this.\n",
    "\n",
    "----\n",
    "\n",
    "```python\n",
    "# By default, Adasum doesn't need scaling up learning rate.\n",
    "lr_scaler = hvd.size() if not args.use_adasum else 1\n",
    "\n",
    "if args.cuda:\n",
    "    # Move model to GPU.\n",
    "    model.cuda()\n",
    "    # If using GPU Adasum allreduce, scale learning rate by local_size.\n",
    "    if args.use_adasum and hvd.nccl_built():\n",
    "        lr_scaler = hvd.local_size()\n",
    "\n",
    "# Horovod: scale learning rate by lr_scaler.\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr * lr_scaler,\n",
    "                      momentum=args.momentum)\n",
    "```\n",
    "\n",
    "Horovod simultaneously trains as many batches as you have GPUs, and the gradient update that is made gets applied to the average of all of these different batch gradients. This means that we can speed up training by multiplying our base learning rate by the number of device could, `hvd.size()`.\n",
    "\n",
    "If you are using the specialty Adasum learning rate scheduler, which this script supports, there are some special rules to follow. See [here](https://horovod.readthedocs.io/en/latest/adasum_user_guide_include.html) in the Horovod docs.\n",
    "\n",
    "----\n",
    "\n",
    "```python\n",
    "# Horovod: broadcast parameters & optimizer state.\n",
    "hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "hvd.broadcast_optimizer_state(optimizer, root_rank=0)\n",
    "```\n",
    "\n",
    "This training script uses default random initialization for the model weights. Each GPU initializes these random weights seperately, so unless we synchronize the initialized weights between machines the training will diverge.\n",
    "\n",
    "The device with the rank of 0 typically has special significance in Horovod: it is the device responsible for this synchronization. These two API calls broadcast the model state from this \"root\" machine to the the other machines in the list, ensuring that they are in sync. Non-root device training scripts will block on this operation until Horovod has performed the sync.\n",
    "\n",
    "----\n",
    "\n",
    "```python\n",
    "# Horovod: (optional) compression algorithm.\n",
    "compression = hvd.Compression.fp16 if args.fp16_allreduce else hvd.Compression.none\n",
    "```\n",
    "\n",
    "Horovod has built-in cross-platform support for performing gradient vector updates in 16-bit floating point. Reduced precision can greatly reduce network overhead by reducing payload sizes by 50% (versus FP32) or more. Since arithmetic precision is not typically a limiting factor in model performance, it's almost always worth using some sort of compression. See [here](https://github.com/horovod/horovod/blob/master/horovod/torch/compression.py) for the implementation.\n",
    "\n",
    "----\n",
    "\n",
    "```python\n",
    "# Horovod: wrap optimizer with DistributedOptimizer.\n",
    "optimizer = hvd.DistributedOptimizer(optimizer,\n",
    "                                     named_parameters=model.named_parameters(),\n",
    "                                     compression=compression,\n",
    "                                     op=hvd.Adasum if args.use_adasum else hvd.Average)\n",
    "```\n",
    "\n",
    "This is where the magic happens. The Horovod `DistributedOptimizer` wrapper takes the optimizer (SGD in this case) as input, delegates gradient computation to it, averages gradients using all-reduce or all-gather, and then applies those averaged gradients across all devices.\n",
    "\n",
    "----\n",
    "\n",
    "```python\n",
    "def metric_average(val, name):\n",
    "    tensor = torch.tensor(val)\n",
    "    avg_tensor = hvd.allreduce(tensor, name=name)\n",
    "    return avg_tensor.item()\n",
    "\n",
    "# ...later...\n",
    "def test():\n",
    "    # Horovod: average metric values across workers.\n",
    "    test_loss = metric_average(test_loss, 'avg_loss')\n",
    "    test_accuracy = metric_average(test_accuracy, 'avg_accuracy')\n",
    "\n",
    "    # Horovod: print output only on first rank.\n",
    "    if hvd.rank() == 0:\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(\n",
    "            test_loss, 100. * test_accuracy))\n",
    "```\n",
    "\n",
    "In the single-machine case, to log the value of a metric we would simply ask for a vector, perform some computation on it, and `print` it.\n",
    "\n",
    "In the multi-machine case things are more complicated. We need each script's local copy of the value of the metric, and then we would need to average these values to get its cluster mean. `hvd.allreduce` returns the average of the local copies of a named vector. If an average is not appropriate, you can use the similar `hvd.allgather` method to collect the vectors into a local list instead, so that you can reduce the values need.\n",
    "\n",
    "For clarity in the logs, we log the results in `test()` on the root machine (`hvd.rank() == 0`) only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running in a spell run\n",
    "\n",
    "Spell runs provide a native integration with Horovod. Use the `--distributed n` flag, where `n` is the number of machines you want to use, to enable Horovod in a run.\n",
    "\n",
    "For testing purposes we recommend starting off with a single-GPU run, e.g. a run with Horovod enabled but no actual model distribution present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spell workspace will not log you in by default. If you are running this notebook from inside of a Spell workspace you will need to run the following command, replacing `YOUR_EMAIL` with your Spell email and `YOUR_PASSWORD` with your Spell password:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mHello, Aleksey Bilogur!\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!spell login --identity YOUR_EMAIL --password YOUR_PASSWORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0müí´ Casting spell #91‚Ä¶\n",
      "\u001b[0m‚ú® Stop viewing logs with ^C\n",
      "\u001b[1m\u001b[36m‚≠ê\u001b[0m Machine_Requested‚Ä¶ Run created -- waiting for a k80 machine.\u001b[0mm^C\n",
      "\n",
      "\u001b[0m‚ú® Your run is still running remotely.\n",
      "\u001b[0m‚ú® Use 'spell kill 91' to terminate your run\n",
      "\u001b[0m‚ú® Use 'spell logs 91' to view logs again\n",
      "\u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!spell run --machine-type K80 \\\n",
    "    --github-url https://github.com/spellrun/examples.git \\\n",
    "    --distributed 1 \\\n",
    "    \"python distributed/pytorch_mnist.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for the distributed experience, a run with `--distributed 2` set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0müí´ Casting spell #93‚Ä¶\n",
      "\u001b[0m‚ú® Stop viewing logs with ^C\n",
      "\u001b[1m\u001b[36müåü\u001b[0m Machine_Requested‚Ä¶ Run created -- waiting for a k80 machine.\u001b[0m^C\n",
      "\n",
      "\u001b[0m‚ú® Your run is still running remotely.\n",
      "\u001b[0m‚ú® Use 'spell kill 93' to terminate your run\n",
      "\u001b[0m‚ú® Use 'spell logs 93' to view logs again\n",
      "\u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!spell run --machine-type K80 \\\n",
    "    --github-url https://github.com/spellrun/examples.git \\\n",
    "    --distributed 2 \\\n",
    "    \"python distributed/pytorch_mnist.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visiting the run page in the web console we can see a couple of ways in which distributed runs are treated differently from regular ones:\n",
    "\n",
    "![](https://i.imgur.com/huQzvq2.png)\n",
    "\n",
    "![](https://i.imgur.com/sWODmav.png)\n",
    "\n",
    "When executing a distributed run on Spell, the machine with the rank 0 GPU is the *distributed primary* and all other machines are *distributed secondaries*. **Only resources from the distributed primary machine will be saved by the run**. This is in conformance with the Horovod way of doing things: all checkpointing should be handled by the host process for the root GPU.\n",
    "\n",
    "On the metrics side, Spell's automatic hardware metrics logging gets extended to every machine, and you will have access to menu options allowing you to configure which GPUs and model metrics appear in the metrics viewer. This same view extends to model metrics as well.\n",
    "\n",
    "One other important difference is the automatic inclusion of the Horovod timeline in the run output. `horovod_timeline.json` is a neat feature allowing you inspect and debug GPU utilization during the training run. You can load this file into `chrome://tracing` to view the timing history of the run:\n",
    "\n",
    "![](https://i.imgur.com/tkB221X.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running in a single spell run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can also use Horovod to distribute the training job across GPUs on the same machine, for example, getting a model training on both GPUs on a K80x2 instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0müí´ Casting spell #92‚Ä¶\n",
      "\u001b[0m‚ú® Stop viewing logs with ^C\n",
      "\u001b[1m\u001b[36m‚≠ê\u001b[0m Machine_Requested‚Ä¶ Run created -- waiting for a k80x2 machine.\u001b[0mm^C\n",
      "\n",
      "\u001b[0m‚ú® Your run is still running remotely.\n",
      "\u001b[0m‚ú® Use 'spell kill 92' to terminate your run\n",
      "\u001b[0m‚ú® Use 'spell logs 92' to view logs again\n",
      "\u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!spell run --machine-type K80x2 \\\n",
    "    --github-url https://github.com/spellrun/examples.git \\\n",
    "    --distributed 1 \\\n",
    "    \"python distributed/pytorch_mnist.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has limited utility simply because the major deep learning libraries have built-in support for data parallelization across GPUs which works well and are much simpler to use than the Horovod API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running in a spell workspace\n",
    "\n",
    "Since Horovod API is baked into our container images, it's possible to use the Horovod toolchain&mdash;specifically, the `horovodrun` CLI command&mdash;to execute a distributed training job from inside of a Spell workspace:\n",
    "\n",
    "```bash\n",
    "$ horovodrun -np 4 -H localhost:4 python pytorch_mnist.py\n",
    "```\n",
    "\n",
    "This command will launch a training job on your local machine spanning four GPUs (an appropriate choice for a `k80x4` or `v100x4` instance). Adjust the value you pass to `-np` and `-H` as needed based on the number of GPUs you have in your instance.\n",
    "\n",
    "This feature is extremely useful for testing and/or debugging your `horovod`-based training scripts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
